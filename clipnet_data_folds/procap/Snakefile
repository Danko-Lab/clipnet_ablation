import json
import os

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define master rule (forces Snakemake to generate all missing files)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

with open("../../data_spec/lcl_proc_spec.json", "r") as f:
    dirs = json.load(f)
    WORKDIR = dirs["WORKDIR"]
    DATADIR = dirs["DATADIR"]
    RAWDIR = dirs["RAWDIR"]
    REFDIR = dirs["REFDIR"]
    FOLDS = 10

fold_assignments = "../data_fold_assignments.csv"

with open("../../data_spec/procap_to_1k_genomes.json", "r") as handle:
    prefix_converter = json.load(handle)
with open("../../data_spec/missing_1k_genomes.txt", "r") as handle:
    missing = handle.read().splitlines()
nonempty_procap_prefixes = [
    prefix
    for prefix in prefix_converter.keys()
    if prefix_converter[prefix] not in missing
]

output = expand(
    os.path.join(DATADIR, "procap/individual_windows/{prefix}.{pm}.{fold}.csv.gz"),
    prefix=nonempty_procap_prefixes,
    pm=["pl", "mn"],
    fold=range(FOLDS),
)


rule procap_all:  # A master rule that ensures all the other rules run
    input:
        output,
    params:
        os.path.join(WORKDIR, "procap"),
    shell:
        "echo rm -r {params}"


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Convert bw files to hg38
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


rule unpack_procap:
    input:
        os.path.join(DATADIR, "procap/bigwig/{prefix}.{pm}.bw"),
    resources:
        load=1,
    output:
        os.path.join(WORKDIR, "procap/bigwig/{prefix}.{pm}.bw"),
    shell:
        "cp {input} {output}"


rule unpack_windows:
    input:
        windows=os.path.join(DATADIR, "procap/windows/{prefix}_window_uniq.bed.gz"),
    resources:
        load=1,
    output:
        windows=os.path.join(WORKDIR, "procap/windows/{prefix}_window_uniq.bed"),
    shell:
        "zcat {input} > {output}"


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Get signal from each window
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


rule procap_get_signal:
    input:
        windows=os.path.join(WORKDIR, "procap/windows/{prefix}_window_uniq.bed"),
        bw=os.path.join(WORKDIR, "procap/bigwig/{prefix}.{pm}.bw"),
    params:
        os.path.join(WORKDIR, "procap/windows/signal/{prefix}.{pm}.txt"),
    resources:
        load=1,
    output:
        os.path.join(WORKDIR, "procap/windows/signal/{prefix}.{pm}.txt.gz"),
    shell:
        """
        bwtool extract bed {input} {params}
        gzip {params}
        """


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Convert signal files to csv (easy to read)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


rule procap_read_signal:
    input:
        os.path.join(WORKDIR, "procap/windows/signal/{prefix}.{pm}.txt.gz"),
    resources:
        load=1,
    output:
        os.path.join(WORKDIR, "procap/windows/signal/{prefix}.{pm}.csv.gz"),
    shell:
        "python ../../data_processing_scripts/read_signal.py {input} | gzip > {output}"


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Split signal files into train, val, and test
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


rule procap_split_signal:
    input:
        os.path.join(WORKDIR, "procap/windows/signal/{prefix}.{pm}.csv.gz"),
    params:
        fold_assignments=fold_assignments,
        fold="{fold}",
    resources:
        load=1,
    output:
        fold=os.path.join(
            DATADIR, "procap/individual_windows/{prefix}.{pm}.{fold}.csv.gz"
        ),
    run:
        import pandas as pd

        fold_assignments = pd.read_csv(
            params.fold_assignments, header=0, index_col=None
        )
        chroms = list(
            fold_assignments[fold_assignments["fold"] == int(params.fold)].chrom
        )
        data = pd.read_csv(input[0], header=None, index_col=0)
        fold = pd.DataFrame(
            [data.loc[idx] for idx in data.index if idx.split(":")[0] in chroms]
        )
        fold.to_csv(output[0], header=False, index=True, compression="gzip")
